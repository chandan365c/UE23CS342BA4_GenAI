{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNUziqKyq7Qv",
        "outputId": "28778c52-8de2-4d0b-fbf3-da9da8772982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "2T1o1uHUrsJ-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_gen = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "roberta_gen = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "bart_gen = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "\n",
        "bert_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "roberta_mask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "bart_mask = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ccef22f7e2a64c3eb86ead843bd9d3dc",
            "7db17a33c3384258a9d6e77a6717eca0",
            "eaed7851baab4a20ba6d327a82559a16",
            "bbd1b41dd8e14b5f8c24098e61bfd285",
            "a5622380f9ce4a64be6a79c1215bff79",
            "cf187d3f80374becb19dbad3560d3cec",
            "430c2c377f554f32b0836c1a09456664",
            "1e1d28e2c54242a8996a740a7131359e",
            "35ba9dfe738446c1bbb465711e9dae69",
            "c366361cc26a4cb183d092b3c6429211",
            "7fe5f80c3f034b15ac98eda920e25dcb",
            "891401ae78094902b137034cf7a79638",
            "80eb1a44239b4ba3a5b41485d49ddf5b",
            "2672ee3de3514548ba7744638c7d0d30",
            "3f1c2283b8f7476b9abaa8806a965b83",
            "b639195fb4604d8d9980d0eb7bfd05a7",
            "0cb6e06aac744c55afa4f32368fde888",
            "b304713b97b743749ed1f1c78ead1802",
            "56d2068a647445b8885fa4e603c4ba3f",
            "f91d9e97ed304865802f971b1bc46ecc",
            "01a4e05e52f24af8a52b23337ebd5519",
            "8546dae413d74d508cf83c7ddc0a2ab0",
            "d2222c22c09c49fca56cc598d821b6ad",
            "bc4d1cc90e1140ffa8cb9c5279930aa9",
            "cda5a17e035b420f988668b943e7a1f0",
            "2e7e1771aa18489ba72dd2995d0f8c2c",
            "000f658edb564dcd9b1b449a319a3a63",
            "396d84b04f06463c8c68724da876b00a",
            "2ed5180a40464224b4ab96addefa906e",
            "3840410b80ad4b8a98b944660ea6c56d",
            "66ea6230879d4648a03b6e0804c599eb",
            "fcadd0bdd8804d61816ead04336578d4",
            "635e4c73d6954e51a8c1851e6be64e8f",
            "23cbd1a3ccc44c6eadf97e22f42e3e66",
            "c2f8d4131dc84b45aec84a621ccfd3e9",
            "ecd204fa6b2646e1905569079e262918",
            "9396a2fdaf8e4bf5929e5da7bef8a07b",
            "2a046076da9141f8a2d6b07a5c3db4d3",
            "3b4b8dae00b5487a9f03287ee084cbf9",
            "e992df40d9974fdb87aa658e9aab4af4",
            "214cab3e0b604e0dbf44c63ed0774623",
            "bbfadc68feb3402fa8bbe91c5876a721",
            "c20099026cc043f294d3f2ca89fc2e52",
            "d04119afc6c843b9b78324f2f0c6bc1f",
            "c05f0ea6da3a4752ab3628de27176619",
            "4ed7bd9c05cf4aed9e6a2bb3f2ca6823",
            "f194f657dafd4c1cb3e55a66ae882fa8",
            "97c0593eddeb4c2fb694e2abf00377b7",
            "6ed8167c67eb46558b86b6b4f9c0725a",
            "369c53ebf86b47579b0b448c0ac2452c",
            "f8537a9eceef4f81ace109e86aec1a0a",
            "bb34db6bf52e40ecaf5a4b09e0b76c49",
            "de69e30385ef40ddbbe4311e45b5efb2",
            "bf55f6736aea4724b7fe3a372043870d",
            "d4d044511fd04a368f8ab8bc6e73bc58",
            "5b1d15c0a26a44b88dfe0757d457ebb5",
            "9a6c48dd788e412aaf5dbcb7f59b5d89",
            "9fdcb1be73af41a8adcada7510f62fd1",
            "82ae7ed8035b4ad4a3c8657496bd7f7b",
            "e95d18891e084c48bd38deb151a3a9be",
            "03f05f2bbee84d47a307548c2d396aa2",
            "feb8c336a8e9484eb0104328a018bf18",
            "cc93e1b1f8b04d0db10237e77573af21",
            "e02fce66190e4e9abdcfd3d953b9fbbd",
            "84163b1e621847ba855b96933e3d114d",
            "135e5a72fd504de18fba20b5b260e6bf",
            "1ef28361bfc64e90a8ba04bddc75de49",
            "14199a610da94d389e02b06886900327",
            "bbbb2493ffd74c0886e3b7d41db38217",
            "37d34f4190eb40d5986d9a3e682f93f8",
            "d8e3b76e9200494a888814b0da87a76c",
            "5e96bb43ee3340eabb1c98da621f3b46",
            "aff47599e99748baa7b6afcf010ff672",
            "beb6611f90ff48e6988c5372f0d072d3",
            "3e2c99a332844c8e8c6288549e1f3343",
            "0985c44d13884dd4a831b129a9a1706b",
            "a1afc74157f7438aa8f47e0845ddf7d9",
            "4c0d7d7a0e88479683d529c01fea6e98",
            "4ae43b14db06475d89c765a3ffcc27dd",
            "158c9a12a643476ab0a77f96dbe04dc0",
            "bd08d82bb49d459ba39bd5e2a40b2a03",
            "9222c4eaaa03415ab78bf4a42c8f7c07",
            "ab101dd2bce6433098f0b4c6ebdbf2e4",
            "cfaae958f4634ddcbdf05e2693cab7e6",
            "1547b5e210094a289f93975deee5a3cf",
            "8dd55a87959a45c980d57f5767e13b0b",
            "3bc8bf8eafa74bf5ab2ae31f7a58a48a",
            "96f7476364a542298de1dc5e88fa7327",
            "63802995126a47a0a7dd914d73a7cc90",
            "bb2ef1ea54234bac86bb204460e14f9b",
            "a4af8709cf3149e8ad9bff8804134710",
            "64760753d52f4adb9d6479f4c4a49214",
            "d6185a65fcd949bda345b756769a902a",
            "caa28508a09040f29411f415cea621ea",
            "d5a034f00c22421386382a6d3afac281",
            "e78f615161fb487bb2613994cd989f6f",
            "d487cfe22c0e430a95e37aa3b85c69ea",
            "49f90a8b9dfd4bf38509d6b942df64b4",
            "7811f4ed866147a3941876c5c7cb1063",
            "f45ef196effe4cef96b0a6be8b6a5744",
            "647b1aaf300c4b3ba02eed99352f6e7a",
            "ce4af87e1663433e99538428a816e11c",
            "231a715c33a144e9ac921fcef67702d9",
            "e012de435ee84d2d906a22e317ac2d6d",
            "78e12636786046e4883209b878ddcc5a",
            "511187f930fa428abf01b7268f879977",
            "963b9e636f604c6cb4d9d23936c7bb2d",
            "090d22f543394694b29c6765a0f937bb",
            "3c8c0b71c6504a8f94323163ae30d926",
            "c96dac98e6d449ae8ff524528e724b5a",
            "b91a698aec2048aca91aa0d5a43817c4",
            "d2b4006c1fdb4af88f4a52bfa83d6225",
            "8a739076342a456281f29b323bf6366e",
            "8d159b9beaa74c1b9012daaa1df6d2a5",
            "b153c268d6884a4aa71fe8b7dd01c2e2",
            "a73ff56f5194444ba39b138f416240b9",
            "b3052fbc5b874cc098b20c5f82ce1697",
            "284bb3cae2a747d3af80bf3ef7875224",
            "0c9d62bcc0ac432294f0896d5cfbacfb",
            "44f4f61a73cc41379f176605250ba8d8",
            "9ed45e6bb18f4f90a102db8ea7faa201",
            "02868a2e31664255938dd42e64f486e3",
            "a63999966e0f474dbb09cadb165e6b68",
            "723ded56fa3a476ba3c01514531d3898",
            "5564eb80cd2d406c84a2f615dbc60029",
            "b679f014429b4a76a906fa9839444af2",
            "4e64044696a94bba91f49f93127dc012",
            "e944b1065cd140a4851898251e359350",
            "77493cd5dd3b4139804e5ea016b8253c",
            "1560f3df82b3460285164d49c91ef1a8",
            "d8a80f59eef5460ebb1f1b8dc388dcbb",
            "b8e0dabad6b14b59832b77d2e6edc5dd",
            "a3c1cb12976f4f0e8f2c4e4ea1f8d79b",
            "471118ecac96479d9bb68fded3fd70c1",
            "296aa17fab0a460da0a57c6967661fe9",
            "07edff4a4c834da6aa12a6b9ce3919e7",
            "accf44bb10b94046977c71b06843407a",
            "231b834668bb4c01ae6eea4a65abcde5",
            "e3fecd8bf6d846a886543744b29170d0",
            "3aceeaa3b2e645d38a31ea389f7552ad",
            "0a5a1fc872cd4b9ea676529c70672e5f",
            "4be4896a669d439491b79839396a1007",
            "d39c8275f94f4b99969d95edbd2a12d8",
            "14575ca4bb774b0e99689c64f4161f5c",
            "6159d370fca4485ea622939518275134",
            "069c470d78d24c929184db5434db597a",
            "4713739c444a44d6add1953753fc3b3c",
            "638edb8fd2c343eba24ac87aa85b554c",
            "8fbb99e6dc0f41389602534dd9f730d8",
            "0689a6fe570a46f183dd9fa6e8f371b4",
            "231c4b8eadf24d13a2824ba7feb3c1a7",
            "20e3d9545dc24eb1ba8521404154d592",
            "58e3937fa0a349049716068dea1a33df",
            "7166aaaf915c4537b103207b66a231d3",
            "1708d3d381714697a599441580b3f9aa",
            "dadf0fa8bf9c48b9bed640a61b8880e4",
            "c9d5a9019d6640888173c822a57d446b",
            "eb1abb34703f474e9374d3c867d5a946",
            "afb2a1f058fa41d7abb5d8b263697106",
            "d1ae0690ed9b43f6ad03b6a0d1728136",
            "bb23e29052914f9e8ef6e0d176cee97e",
            "367e9f0c9d4147d6944280e56bc0ed27",
            "f1709c854bd14cd9adffa963a7f33074",
            "287b8b9f33d64c3c9a40f9b4c222c403",
            "923eed8791b54ff485233e30788719d5",
            "72360c28c8b14a3eb7e2dcb0626de0ea",
            "2a6c120e582b449a89879bbe79874f18",
            "56b2aebb9b3e4e218be7c4ced55aea9a",
            "3d3ad473f15a432aa5387191e566b813",
            "a8b8a083b7284df5948af4bd63d07f0b",
            "efd6b9862cb844e493c38422c398016e",
            "bf0299635ec448498743fabd313e93c5",
            "1b9113b5a4884567a313b08a364b13a0",
            "cfe8fc856f744d8d8b41a653fa5bb47d",
            "16d1bd13116e45e2a1d33eaff14c0968",
            "bcdfdf8fcdd4429d885dbd4e86153b4e"
          ]
        },
        "id": "WlR_KbbfsHte",
        "outputId": "11febd1d-b5dd-46af-9c4f-3ff514795b2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccef22f7e2a64c3eb86ead843bd9d3dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891401ae78094902b137034cf7a79638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2222c22c09c49fca56cc598d821b6ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23cbd1a3ccc44c6eadf97e22f42e3e66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c05f0ea6da3a4752ab3628de27176619"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b1d15c0a26a44b88dfe0757d457ebb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ef28361bfc64e90a8ba04bddc75de49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c0d7d7a0e88479683d529c01fea6e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63802995126a47a0a7dd914d73a7cc90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45ef196effe4cef96b0a6be8b6a5744"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b91a698aec2048aca91aa0d5a43817c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02868a2e31664255938dd42e64f486e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3c1cb12976f4f0e8f2c4e4ea1f8d79b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14575ca4bb774b0e99689c64f4161f5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1708d3d381714697a599441580b3f9aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72360c28c8b14a3eb7e2dcb0626de0ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 1:"
      ],
      "metadata": {
        "id": "HQyyz-Grsr52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "print(\"BERT:\", bert_gen(prompt))\n",
        "print(\"RoBERTa:\", roberta_gen(prompt))\n",
        "print(\"BART:\", bart_gen(prompt, max_length=40))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjpN-pJ0sidJ",
        "outputId": "253f40d4-3727-49f2-8d8e-24e08d95c982"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: [{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n",
            "RoBERTa: [{'generated_text': 'The future of Artificial Intelligence is'}]\n",
            "BART: [{'generated_text': 'The future of Artificial Intelligence is significantly significantly significantlynex////////////////////////////////awk devicesiaries Baliaries circuitiaries Sooniaries////////////////////////////////iaries illeg significantlyiariesQiariesiariesiariesinsteiniariesiaries residencyiariesiaries Manifestiaries embarrassment embarrassmentJanJan embarrassment residencyJan embarrassment embarrassment reson embarrassment Moment embarrassment embarrassment parodyJan embarrassment pans devices embarrassmentJan embarrassmentakeningJan trance Washington embarrassment embarrassment residency embarrassment embarrassment embarrassment trance embarrassment embarrassment 444 raged embarrassment publicationJan residency embarrassmentJan Ast embarrassment parents embarrassment embarrassmentective embarrassmentJan parents embarrassment punish embarrassment embarrassment Nicholas embarrassment embarrassment Ast embarrassment embarrassment raged embarrassment Hi embarrassment embarrassment publication embarrassment embarrassmentdx embarrassment embarrassment Conf embarrassment embarrassment undrafted embarrassment embarrassment • embarrassment embarrassmentsignificant embarrassment embarrassment}\\\\ embarrassment embarrassmentromising embarrassment embarrassmentALS embarrassment embarrassmentairo embarrassment embarrassmentetrical embarrassment embarrassment namely embarrassment embarrassmentreference embarrassment embarrassment consultants embarrassment embarrassment cap embarrassment embarrassmentvice embarrassment embarrassmentoni embarrassment embarrassment drawbacks embarrassment embarrassment guilt embarrassment embarrassment relational embarrassment embarrassmentoub embarrassment embarrassment situ embarrassment embarrassment Brewery embarrassment embarrassmentMarie embarrassment embarrassment insects embarrassment embarrassment Awakening embarrassment embarrassment explodes embarrassment embarrassment warning embarrassment embarrassment phenomena embarrassment embarrassmentggie embarrassment embarrassmentousand embarrassment embarrassment� embarrassment embarrassment impending embarrassment embarrassment refurb embarrassment embarrassment cloud embarrassment embarrassment allergies embarrassment embarrassmentospons embarrassment embarrassmentumbs embarrassment embarrassment ur embarrassment embarrassment refunds embarrassment embarrassment pesticide embarrassment embarrassment scar embarrassment embarrassmentsession embarrassment embarrassment immortality embarrassment embarrassment farther 192 embarrassment embarrassment response embarrassment embarrassmentNAT embarrassment embarrassment Gutenberg embarrassment embarrassmenterr embarrassment embarrassment corro embarrassment embarrassment majors embarrassment embarrassment LINK embarrassment embarrassment'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 2:"
      ],
      "metadata": {
        "id": "azAOAPQZs3cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "sentence_roberta_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "\n",
        "print(\"BERT:\", bert_mask(sentence_bert))\n",
        "print(\"RoBERTa:\", roberta_mask(sentence_roberta_bart))\n",
        "print(\"BART:\", bart_mask(sentence_roberta_bart))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6igNW5Hxs5K0",
        "outputId": "558c06b6-89b9-4b11-c223-4aff4587cae4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: [{'score': 0.5396888852119446, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}, {'score': 0.15575668215751648, 'token': 9699, 'token_str': 'generate', 'sequence': 'the goal of generative ai is to generate new content.'}, {'score': 0.054054468870162964, 'token': 3965, 'token_str': 'produce', 'sequence': 'the goal of generative ai is to produce new content.'}, {'score': 0.04451529309153557, 'token': 4503, 'token_str': 'develop', 'sequence': 'the goal of generative ai is to develop new content.'}, {'score': 0.01757732406258583, 'token': 5587, 'token_str': 'add', 'sequence': 'the goal of generative ai is to add new content.'}]\n",
            "RoBERTa: [{'score': 0.3711293935775757, 'token': 5368, 'token_str': ' generate', 'sequence': 'The goal of Generative AI is to generate new content.'}, {'score': 0.36771273612976074, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.08351442217826843, 'token': 8286, 'token_str': ' discover', 'sequence': 'The goal of Generative AI is to discover new content.'}, {'score': 0.021335095167160034, 'token': 465, 'token_str': ' find', 'sequence': 'The goal of Generative AI is to find new content.'}, {'score': 0.016521504148840904, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}]\n",
            "BART: [{'score': 0.0746147632598877, 'token': 1045, 'token_str': ' create', 'sequence': 'The goal of Generative AI is to create new content.'}, {'score': 0.06571780890226364, 'token': 244, 'token_str': ' help', 'sequence': 'The goal of Generative AI is to help new content.'}, {'score': 0.060879286378622055, 'token': 694, 'token_str': ' provide', 'sequence': 'The goal of Generative AI is to provide new content.'}, {'score': 0.03593532741069794, 'token': 3155, 'token_str': ' enable', 'sequence': 'The goal of Generative AI is to enable new content.'}, {'score': 0.03319435939192772, 'token': 1477, 'token_str': ' improve', 'sequence': 'The goal of Generative AI is to improve new content.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fixrrjQ6tKpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 3:"
      ],
      "metadata": {
        "id": "Et7yKLX0tIm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "print(\"BERT:\", bert_qa(question=question, context=context))\n",
        "print(\"RoBERTa:\", roberta_qa(question=question, context=context))\n",
        "print(\"BART:\", bart_qa(question=question, context=context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvCmkHa2tGMY",
        "outputId": "a2256819-2359-4bf7-b464-7272a37ebfbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: {'score': 0.013879857491701841, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n",
            "RoBERTa: {'score': 0.0043233539909124374, 'start': 72, 'end': 82, 'answer': 'deepfakes.'}\n",
            "BART: {'score': 0.05483523663133383, 'start': 72, 'end': 81, 'answer': 'deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|------|-------|----------------------------------|----------------------------------------|--------------------------------------------|\n",
        "| Generation | BERT | Failure | Could not generate a meaningful continuation; produced errors or incoherent text. | BERT is an encoder; it isn't trained to predict the next word |\n",
        "|  | RoBERTa | Failure | Output was nonsensical and the pipeline failed to generate proper text. | RoBERTa is also encoder-only and lacks a decoder for sequence generation. |\n",
        "|  | BART | Success | Generated a fluent continuation of the prompt. | BART has an encoder–decoder architecture trained for sequence-to-sequence generation. |\n",
        "| Fill-Mask | BERT | Success | Predicted 'create', 'generate'. | BERT is trained on Masked Language Modeling (MLM). |\n",
        "|  | RoBERTa | Success | Predicted highly accurate and confident masked tokens. | RoBERTa is an optimized encoder trained extensively on MLM. |\n",
        "|  | BART | Partial Success | Filled the mask but predictions were less direct or varied. | BART is trained with denoising autoencoding, not pure MLM. |\n",
        "| QA | BERT | Partial Success | Extracted relevant phrase such as “hallucinations, bias, and deepfakes” with low confidence. | Base BERT is not fine-tuned for question answering tasks. |\n",
        "|  | RoBERTa | Partial Success | Returned similar or slightly better span selection than BERT. | Good encoder, but still specifically trained for QA purposes. |\n",
        "|  | BART | Partial Success | Sometimes produced the correct answer span, sometimes incomplete. | Encoder–decoder can correctly map question-context pairs, but without fine-tuning properly for QA, its performance is unstable. |"
      ],
      "metadata": {
        "id": "U1p5ExIRyCGr"
      }
    }
  ]
}
